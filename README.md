# ml-project-blueprint

# ğŸš² Bike Rental Demand Forecasting System

## A Machine Learning Application that forecasts hour-ahead bike rental demand across an entire city, enabling dynamic pricing optimization and revenue maximization.

![User Interface](images/user-interface.png)

## ğŸ¯ Executive Summary

This **Real-Time Bike Rental Demand Forecasting System** solves a critical business problem for bike-sharing platforms operating across entire cities. The system provides hour-ahead demand predictions that drive dynamic pricing strategies, enabling revenue optimization and improved fleet utilization.


## Business Problem Solved

**Challenge**: Bike-sharing platforms need to optimize dynamic pricing based on predicted demand to:
- **Maximize Revenue**: Increase prices during high-demand surges
- **Stimulate Demand**: Lower prices during slow periods to attract riders
- **Optimize Fleet**: Better plan supply-demand balance and bike distribution

**Solution**: A real-time ML system that forecasts city-wide bike rental demand for the next hour, enabling:
- **Dynamic Pricing Optimization**: Real-time price adjustments based on demand forecasts
- **Incentive Campaigns**: Targeted promotions during predicted low-demand periods
- **Operational Planning**: Improved fleet utilization and supply-demand balance

## Application architecture
The architecture of the system is shown below:

![ML Pipeline](images/ml-pipeline-architecture.png)

### End-to-End ML Pipeline Architecture

The system implements a comprehensive machine learning pipeline with five distinct stages:

#### 1. **Preprocessing Pipeline**
- **Column Renaming**: Configurable column mapping for data standardization
- **Column Dropping**: Removal of unnecessary columns based on configuration
- **Data Reset**: Index reset for clean data structure
- **Configuration-Driven**: All preprocessing steps controlled via config parameters

#### 2. **Feature Engineering Pipeline**
- **Lag Features**: Creation of time-lagged features for time series forecasting
- **Configurable Lags**: Multiple lag periods (e.g., 1, 2, 3, 5, 10 periods) for different features
- **Backward Fill**: Handling of missing values in lag features using bfill method
- **Feature Naming**: Automatic naming convention: `{feature}_lag_{period}`

#### 3. **Training Pipeline**
- **Target Creation**: Shifted target variable for forecasting (configurable shift period)
- **Time-Series Split**: Train-test split without shuffling to preserve temporal order
- **CatBoost Model**: Gradient boosting regressor with configurable parameters
- **Optuna Hyperparameter Tuning**: Automated optimization of learning_rate, depth, and l2_leaf_reg
- **Early Stopping**: Prevents overfitting with configurable early stopping rounds
- **Time-Based Validation**: Manual validation split within training data

#### 4. **Inference Pipeline**
- **Model Loading**: Dynamic model loading from specified path
- **Single Prediction**: Returns only the last prediction value for real-time forecasting
- **Feature Preparation**: Input DataFrame processing for model inference
- **Timestamp Integration**: Current timestamp handling for prediction tracking

#### 5. **Postprocessing Pipeline**
- **Model Persistence**: Automated model saving after training
- **Prediction Formatting**: Single-row DataFrame with timestamp and prediction columns
- **Time Increment**: Configurable time increment for prediction timestamps
- **Data Structure**: Standardized output format for downstream processing

### ğŸ³ Entrypoints
- **`app-ml/train.py`**: Model training entrypoint boith locally and in production
- **`app-ml/inference.py`** Entrypoint to run inference pipeline locally
- **`app-ml/inference-api.py`**: API for inference in production / on web-app
- **`app-ui/app.py`**: Interactive dashboard for demand reocasting monitoring

---

## ğŸš€ Quick Start

### Option 1: Running Application in Docker (Recommended)

```bash
# Clone the repository
git clone <your-repo-url>
cd ml-project-blueprint

# Deploy all services with production configuration
docker-compose up --build

# Verify service health
docker-compose ps
docker-compose logs -f

# Access the application
# ğŸŒ UI Dashboard: http://localhost:8050
# ğŸ”Œ Inference API: http://localhost:5001/health
```

**Expected Output:**
```
âœ… app-ml-train         Up
âœ… app-ml-inference-api Up  
âœ… app-ui              Up
```

### Option 2: Running Application Locally

```bash
# Clone the repository
git clone <your-repo-url>
cd ml-project-blueprint

# Create and activate conda environment
conda env create -f environment.yml
conda activate ml-blueprint


# Train the model first (if not already trained)
python app-ml/entrypoint/train.py

# Run inference in a loop 
python app-ml/entrypoint/inference.py

# Start the inference API tomlink to the application UI
python app-ml/entrypoint/inference_api.py

# Start the UI dashboard in another terminal
cd app-ui
python app.py
```

**Access the application:**
- ğŸŒ **UI Dashboard**: http://localhost:8050
- ğŸ”Œ **Inference API**: http://localhost:5001


## ğŸ“ Project Architecture & Data Flow

```
ml-project-blueprint/
â”œâ”€â”€ ğŸ“ app-ml/                           # Demand Forecasting Engine
â”‚   â”œâ”€â”€ ğŸ“ entrypoint/                  # Production ML Services
â”‚   â”‚   â”œâ”€â”€ prod_train.py               # Demand model training pipeline
â”‚   â”‚   â”œâ”€â”€ prod_inference.py           # Batch demand prediction service
â”‚   â”‚   â””â”€â”€ inference_api.py            # Real-time demand prediction API
â”‚   â”œâ”€â”€ ğŸ“ notebooks/                   # Data Science & Analysis
â”‚   â”‚   â”œâ”€â”€ EDA.ipynb                   # Demand pattern analysis
â”‚   â”‚   â””â”€â”€ Modeling.ipynb              # Demand forecasting model development
â”‚   â”œâ”€â”€ ğŸ“ src/                         # Core Forecasting Pipeline
â”‚   â”‚   â”œâ”€â”€ ğŸ“ pipelines/               # Modular demand forecasting components
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # Rental data preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_engineering.py  # Weather & temporal feature creation
â”‚   â”‚   â”‚   â”œâ”€â”€ training.py             # Demand model training pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py            # Real-time demand prediction
â”‚   â”‚   â”‚   â””â”€â”€ postprocessing.py       # Pricing optimization logic
â”‚   â”‚   â””â”€â”€ utils.py                    # Forecasting utilities & helpers
â”‚   â”œâ”€â”€ Dockerfile                      # ML service containerization
â”‚   â””â”€â”€ requirements.txt                # ML dependencies
â”œâ”€â”€ ğŸ“ app-ui/                          # Dynamic Pricing Dashboard
â”‚   â”œâ”€â”€ app.py                          # Main pricing dashboard application
â”‚   â”œâ”€â”€ assets/                         # Dashboard styling & assets
â”‚   â”œâ”€â”€ Dockerfile                      # UI service containerization
â”‚   â””â”€â”€ requirements.txt                # UI dependencies
â”œâ”€â”€ ğŸ“ common/                          # Shared Business Logic
â”‚   â”œâ”€â”€ data_manager.py                 # Rental data management & persistence
â”‚   â””â”€â”€ utils.py                        # Common utilities & pricing helpers
â”œâ”€â”€ ğŸ“ config/                          # Configuration Management
â”‚   â”œâ”€â”€ local.yaml                      # Development configuration
â”‚   â”œâ”€â”€ staging.yaml                    # Staging environment config
â”‚   â””â”€â”€ production.yaml                 # Production environment config
â”œâ”€â”€ ğŸ“ data/                            # Bike Rental Data Lake
â”‚   â”œâ”€â”€ ğŸ“ raw_data/                   # Raw rental & weather data
â”‚   â”‚   â”œâ”€â”€ csv/                       # Historical rental data (CSV)
â”‚   â”‚   â””â”€â”€ parquet/                   # Optimized rental data (Parquet)
â”‚   â””â”€â”€ ğŸ“ prod_data/                  # Processed data & predictions
â”‚       â”œâ”€â”€ csv/                       # Demand predictions (CSV)
â”‚       â””â”€â”€ parquet/                   # Demand predictions (Parquet)
â”œâ”€â”€ ğŸ“ models/                          # Demand Forecasting Models
â”‚   â”œâ”€â”€ ğŸ“ experiments/                # Model experimentation & A/B testing
â”‚   â””â”€â”€ ğŸ“ prod/                       # Production demand forecasting models
â”œâ”€â”€ ğŸ“ images/                          # Documentation & Visualizations
â”œâ”€â”€ docker-compose.yml                 # Multi-service orchestration
â”œâ”€â”€ environment.yml                    # Conda environment specification
â””â”€â”€ README.md                          # Project documentation
```

## License

This project is licensed under a custom **Personal Use License**.

You are free to:
- Use the code for personal or educational purposes
- Publish your own fork or modified version on GitHub **with attribution**

You are **not allowed to**:
- Use this code or its derivatives for commercial purposes
- Resell or redistribute the code as your own product
- Remove or change the license or attribution

For any use beyond personal or educational purposes, please contact the author for written permission.

</div> 



#----- issue while running Jupyter notebook locally ---
    # 1. Check what kernels Jupyter thinks are running

    # In a terminal:

    jupyter server list


    # If thereâ€™s more than one server running and youâ€™re connecting to an old one, kill all of them and start a single clean       one.

    # In the browser (JupyterLab):
    # File â†’ Shut Down â†’ shut down all kernels and terminals.

    # 2. Make sure your environment isnâ€™t half-broken

    # Youâ€™re running Python 3.14 according to that path:

    # .../Python.framework/Versions/3.14/lib/python3.14/...


    # Thatâ€™s bleeding-edge. Some Jupyter stack pieces may be buggy or not fully tested against it.

    # If weirdness persists:

    # Create a clean venv with a mainstream Python (3.11 or 3.12).

    Install Jupyter there:

    python -m venv .venv
    source .venv/bin/activate  # Windows: .venv\Scripts\activate
    pip install --upgrade pip
    pip install jupyterlab ipykernel
    python -m ipykernel install --user --name clean-env --display-name "Python (clean-env)"
    jupyter lab


    # In the UI, pick the Python (clean-env) kernel.

    # If the error disappears in the clean env, your old environment is just messy.
# --- fix-------
